{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings                                              \n",
    "from sklearn.exceptions import DataConversionWarning          \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
      "0        2       3  12669  9656     7561     214              2674        1338\n",
      "1        2       3   7057  9810     9568    1762              3293        1776\n",
      "2        2       3   6353  8808     7684    2405              3516        7844\n",
      "3        1       3  13265  1196     4221    6404               507        1788\n",
      "4        2       3  22615  5410     7198    3915              1777        5185\n"
     ]
    }
   ],
   "source": [
    "# loading the data: Wholesale customers data\n",
    "customers = pd.read_csv('../data/Wholesale customers data.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(customers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Details:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 440 entries, 0 to 439\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Channel           440 non-null    int64  \n",
      " 1   Region            440 non-null    int64  \n",
      " 2   Fresh             440 non-null    float64\n",
      " 3   Milk              440 non-null    float64\n",
      " 4   Grocery           440 non-null    float64\n",
      " 5   Frozen            440 non-null    float64\n",
      " 6   Detergents_Paper  440 non-null    float64\n",
      " 7   Delicassen        440 non-null    float64\n",
      " 8   Total_Spend       440 non-null    float64\n",
      " 9   labels            440 non-null    int32  \n",
      "dtypes: float64(7), int32(1), int64(2)\n",
      "memory usage: 32.8 KB\n",
      "None\n",
      "\n",
      "Categorical Columns: ['Channel', 'Region']\n",
      "\n",
      "Missing Data per Column:\n",
      "Channel             0\n",
      "Region              0\n",
      "Fresh               0\n",
      "Milk                0\n",
      "Grocery             0\n",
      "Frozen              0\n",
      "Detergents_Paper    0\n",
      "Delicassen          0\n",
      "Total_Spend         0\n",
      "labels              0\n",
      "dtype: int64\n",
      "\n",
      "Correlation Matrix:\n",
      "                   Channel    Region     Fresh      Milk   Grocery    Frozen  \\\n",
      "Channel           1.000000  0.062028 -0.173028  0.502382  0.671661 -0.242080   \n",
      "Region            0.062028  1.000000  0.050138  0.022971  0.000457 -0.022097   \n",
      "Fresh            -0.173028  0.050138  1.000000  0.052198 -0.051910  0.381219   \n",
      "Milk              0.502382  0.022971  0.052198  1.000000  0.767125  0.072928   \n",
      "Grocery           0.671661  0.000457 -0.051910  0.767125  1.000000 -0.086003   \n",
      "Frozen           -0.242080 -0.022097  0.381219  0.072928 -0.086003  1.000000   \n",
      "Detergents_Paper  0.689107 -0.003976 -0.136264  0.691414  0.903659 -0.167969   \n",
      "Delicassen        0.144665  0.031437  0.278835  0.448538  0.303369  0.277966   \n",
      "Total_Spend       0.402312  0.030776  0.571826  0.769382  0.738399  0.335017   \n",
      "labels            0.729320  0.054857 -0.203324  0.582103  0.680058 -0.252205   \n",
      "\n",
      "                  Detergents_Paper  Delicassen  Total_Spend    labels  \n",
      "Channel                   0.689107    0.144665     0.402312  0.729320  \n",
      "Region                   -0.003976    0.031437     0.030776  0.054857  \n",
      "Fresh                    -0.136264    0.278835     0.571826 -0.203324  \n",
      "Milk                      0.691414    0.448538     0.769382  0.582103  \n",
      "Grocery                   0.903659    0.303369     0.738399  0.680058  \n",
      "Frozen                   -0.167969    0.277966     0.335017 -0.252205  \n",
      "Detergents_Paper          1.000000    0.171181     0.633648  0.627374  \n",
      "Delicassen                0.171181    1.000000     0.529470  0.274903  \n",
      "Total_Spend               0.633648    0.529470     1.000000  0.407870  \n",
      "labels                    0.627374    0.274903     0.407870  1.000000  \n",
      "\n",
      "\n",
      "Descriptive Statistics:\n",
      "          Channel      Region         Fresh          Milk       Grocery  \\\n",
      "count  440.000000  440.000000    440.000000    440.000000    440.000000   \n",
      "mean     1.322727    2.543182  11797.643295   5641.214318   7715.149318   \n",
      "std      0.468052    0.774272  11556.065072   6382.145852   8103.786401   \n",
      "min      1.000000    1.000000      3.000000     55.000000      3.000000   \n",
      "25%      1.000000    2.000000   3127.750000   1533.000000   2153.000000   \n",
      "50%      1.000000    3.000000   8504.000000   3627.000000   4755.500000   \n",
      "75%      2.000000    3.000000  16933.750000   7190.250000  10655.750000   \n",
      "max      2.000000    3.000000  56082.610000  37610.060000  43435.740000   \n",
      "\n",
      "             Frozen  Detergents_Paper   Delicassen    Total_Spend      labels  \n",
      "count    440.000000        440.000000   440.000000     440.000000  440.000000  \n",
      "mean    2891.643409       2787.916023  1387.448409   32221.014773    0.434091  \n",
      "std     3537.152169       4203.651451  1470.885271   22129.628704    0.496201  \n",
      "min       25.000000          3.000000     3.000000     904.000000    0.000000  \n",
      "25%      742.250000        256.750000   408.250000   17448.750000    0.000000  \n",
      "50%     1526.000000        816.500000   965.500000   27492.000000    0.000000  \n",
      "75%     3554.250000       3922.000000  1820.250000   40660.500000    1.000000  \n",
      "max    17964.820000      22571.610000  8274.660000  162330.410000    1.000000  \n",
      "\n",
      "Pareto Analysis: Top 20% of customers contribute to 83.49% of total spending.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column Information\n",
    "print(\"Column Details:\")\n",
    "print(customers.info())\n",
    "\n",
    "# Step 2: Categorical Data Check\n",
    "categorical_columns = ['Channel', 'Region']\n",
    "print(\"\\nCategorical Columns:\", categorical_columns)\n",
    "\n",
    "# Check for Missing Data\n",
    "missing_data = customers.isnull().sum()\n",
    "print(\"\\nMissing Data per Column:\")\n",
    "print(missing_data)\n",
    "\n",
    "# Step 4: Correlation Analysis\n",
    "correlation_matrix = customers.corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "print()\n",
    "\n",
    "# Descriptive Statistics\n",
    "descriptive_stats = customers.describe()\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(descriptive_stats)\n",
    "\n",
    "# Pareto Analysis\n",
    "# Calculate total spending per customer\n",
    "customers['Total_Spend'] = customers[['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']].sum(axis=1)\n",
    "\n",
    "# Sort customers by total spending in descending order\n",
    "pareto_data = customers[['Total_Spend']].sort_values(by='Total_Spend', ascending=False)\n",
    "\n",
    "# Calculate cumulative percentage of spending\n",
    "pareto_data['Cumulative_Spend'] = pareto_data['Total_Spend'].cumsum()\n",
    "pareto_data['Cumulative_Percentage'] = pareto_data['Cumulative_Spend'] / pareto_data['Total_Spend'].sum() * 100\n",
    "\n",
    "# Identify the point where 20% of customers contribute to 80% of the spend\n",
    "pareto_20_percent_index = int(0.2 * len(pareto_data))\n",
    "pareto_80_percent_spend = pareto_data.loc[:pareto_20_percent_index, 'Cumulative_Percentage'].max()\n",
    "\n",
    "print(f\"\\nPareto Analysis: Top 20% of customers contribute to {pareto_80_percent_spend:.2f}% of total spending.\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Distribution\n",
    "customers.hist(figsize=(12, 10), bins=20, edgecolor='black')\n",
    "plt.suptitle('Data Distribution', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,               # Mostrar los valores en el heatmap\n",
    "    fmt=\".2f\",                # Formato de los números\n",
    "    cmap=\"coolwarm\",          # Esquema de colores\n",
    "    cbar=True,                # Mostrar barra de color\n",
    "    xticklabels=correlation_matrix.columns,  # Etiquetas en los ejes\n",
    "    yticklabels=correlation_matrix.columns,\n",
    "    square=True               # Celdas cuadradas\n",
    ")\n",
    "plt.title(\"Correlation Matrix Heatmap\", pad=20)\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotar las etiquetas del eje x\n",
    "plt.yticks(rotation=0)               # Mantener las etiquetas del eje y horizontales\n",
    "plt.tight_layout()                   # Ajustar los elementos para que no se solapen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection with Boxplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "customers.boxplot(column=['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen'])\n",
    "plt.title('Boxplots for Numerical Features')\n",
    "plt.ylabel('Spending')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "\n",
    "- Dataset Overview: Contains 440 records and 9 columns with no missing values.\n",
    "- Column Characteristics: Channel and Region are categorical (integer-encoded); other columns are numerical and represent spending in various categories.\n",
    "- Data Distribution: Numerical columns are highly right-skewed, while Channel and Region show binary and ternary distributions.\n",
    "- Outliers: Most columns have significant outliers (e.g., high spending in Fresh, Milk, and Grocery), requiring potential treatment.\n",
    "- Correlation Analysis:\n",
    "    - Strong correlations suggest redundancy (e.g., Grocery and Detergents_Paper at 0.92), possibly addressed with PCA or feature selection.\n",
    "    - Weaker correlations for Fresh, Frozen, and Delicassen indicate unique contributions.\n",
    "- Pareto Principle: 20% of customers contribute 84% of total revenue, reflecting the Pareto principle.\n",
    "- Descriptive Statistics: Spending ranges widely across categories, with high standard deviations confirming extreme values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows after cleaning and transformation:\n",
      "   Channel  Region      Fresh      Milk   Grocery    Frozen  Detergents_Paper  \\\n",
      "0        2       3   9.446992  9.175438  8.930891  5.370638          7.891705   \n",
      "1        2       3   8.861917  9.191259  9.166284  7.474772          8.099858   \n",
      "2        2       3   8.756840  9.083529  8.947026  7.785721          8.165364   \n",
      "3        1       3   9.492960  7.087574  8.348064  8.764834          6.230481   \n",
      "4        2       3  10.026413  8.596189  8.881697  8.272826          7.483244   \n",
      "\n",
      "   Delicassen  Total_Spend  \n",
      "0    7.199678        34112  \n",
      "1    7.482682        33266  \n",
      "2    8.967632        36610  \n",
      "3    7.489412        27381  \n",
      "4    8.553718        46100  \n"
     ]
    }
   ],
   "source": [
    "# Outlier Treatment (Capping at 99th percentile)\n",
    "numeric_columns = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "for col in numeric_columns:\n",
    "    upper_limit = np.percentile(customers[col], 99)  # 99th percentile\n",
    "    customers[col] = np.where(customers[col] > upper_limit, upper_limit, customers[col])\n",
    "\n",
    "# Log Transformation to Reduce Skewness\n",
    "# Apply log transformation to numerical columns\n",
    "data_log_transformed = customers.copy()\n",
    "for col in numeric_columns:\n",
    "    data_log_transformed[col] = np.log1p(customers[col])  # log(1 + x) to handle zeros\n",
    "\n",
    "# Check transformed data\n",
    "print(\"First 5 rows after cleaning and transformation:\")\n",
    "print(data_log_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Transformation - Rationale\n",
    "- Outliers:\n",
    "    - Significant outliers exist in the dataset (as shown in the boxplots). These can disproportionately affect distance-based algorithms like clustering.\n",
    "    - Handling outliers (e.g., capping or scaling) can stabilize data variability.\n",
    "- Skewed Distributions:\n",
    "    - Many features (e.g., \"Fresh\", \"Frozen\", \"Milk\") exhibit highly skewed distributions.\n",
    "    - Log or power transformations can help reduce skewness and improve model performance.\n",
    "- Collinearity:\n",
    "    - Strong correlations exist between features like \"Grocery\" and \"Detergents_Paper\" (0.92). This could introduce redundancy in models and distort clustering or regression results.\n",
    "    - Dimensionality reduction (e.g., PCA) can address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation - Comments:\n",
    "- Outlier Treatment: Outliers capped at the 99th percentile for each column to ensure extreme values don’t dominate the analysis.\n",
    "- Log Transformation: Applied to reduce skewness and stabilize variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling:\n",
    "Features like \"Fresh\" and \"Milk\" have ranges varying from single digits to over 100,000, making scaling essential for distance-based methods like K-means or hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the Data\n",
    "scaler = StandardScaler()\n",
    "customers_scale = scaler.fit_transform(data_log_transformed[numeric_columns])\n",
    "\n",
    "# Dimensionality Reduction (Optional: PCA to address collinearity)\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components for visualization\n",
    "pca_transformed = pca.fit_transform(customers_scale)\n",
    "\n",
    "# Explained Variance Ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_transformed[:, 0], pca_transformed[:, 1], alpha=0.6)\n",
    "plt.title('PCA - First 2 Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Summary of Transformation\n",
    "print(\"PCA Explained Variance Ratio:\", explained_variance)\n",
    "print(\"Data transformation completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation - Comments:\n",
    "- Feature Scaling: StandardScaler was used to ensure all features are on the same scale for distance-based algorithms.\n",
    "- Dimensionality Reduction (Optional):\n",
    "    - PCA reduced dimensions to two components for visualization and to address feature collinearity.\n",
    "    - The PCA explained variance ratio quantifies how much information is retained in the reduced dimensions.\n",
    "### Data Preprocessing - Comments\n",
    "- Feature Scaling: StandardScaler was used to ensure all features are on the same scale for distance-based algorithms.\n",
    "\n",
    "- Data Preprocessing includes scaling features for equal contribution to clustering or distance-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels Assigned to Customers:\n",
      "   labels\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       0\n",
      "4       1\n",
      "\n",
      "Cluster Distribution:\n",
      "labels\n",
      "0    249\n",
      "1    191\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the cleaned and preprocessed dataset (scaled data)\n",
    "# Assuming 'customers_scale' is the scaled data from preprocessing\n",
    "customers_scale = pd.DataFrame(customers_scale, columns=numeric_columns)\n",
    "\n",
    "# Step 1: Initialize the K-Means model\n",
    "# Based on the previous analysis the clusters will be 2\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "\n",
    "# Step 2: Fit the K-Means model to the scaled data\n",
    "kmeans.fit(customers_scale)\n",
    "\n",
    "# Step 3: Retrieve the cluster labels\n",
    "customers_scale['labels'] = kmeans.labels_\n",
    "\n",
    "# Step 4: Assign labels back to the original data (optional)\n",
    "# Assuming 'data' is the original dataset\n",
    "customers['labels'] = kmeans.labels_\n",
    "\n",
    "# Step 5: Display the results\n",
    "print(\"Cluster Labels Assigned to Customers:\")\n",
    "print(customers[['labels']].head())\n",
    "\n",
    "# Step 6: Analyze Cluster Distribution\n",
    "print(\"\\nCluster Distribution:\")\n",
    "print(customers['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters using PCA (2D projection of scaled data)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a scatter plot of the first two main features\n",
    "sns.scatterplot(\n",
    "    x=customers_scale['Fresh'], \n",
    "    y=customers_scale['Milk'], \n",
    "    hue=customers_scale['labels'], \n",
    "    palette=\"viridis\", \n",
    "    s=100, \n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.title('K-Means Clustering Results', fontsize=16)\n",
    "plt.xlabel('Fresh (Scaled)', fontsize=12)\n",
    "plt.ylabel('Milk (Scaled)', fontsize=12)\n",
    "plt.legend(title='Cluster', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "      <th>Total_Spend</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669.0</td>\n",
       "      <td>9656.0</td>\n",
       "      <td>7561.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>2674.0</td>\n",
       "      <td>1338.0</td>\n",
       "      <td>34112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>9810.0</td>\n",
       "      <td>9568.0</td>\n",
       "      <td>1762.0</td>\n",
       "      <td>3293.0</td>\n",
       "      <td>1776.0</td>\n",
       "      <td>33266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353.0</td>\n",
       "      <td>8808.0</td>\n",
       "      <td>7684.0</td>\n",
       "      <td>2405.0</td>\n",
       "      <td>3516.0</td>\n",
       "      <td>7844.0</td>\n",
       "      <td>36610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265.0</td>\n",
       "      <td>1196.0</td>\n",
       "      <td>4221.0</td>\n",
       "      <td>6404.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>27381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615.0</td>\n",
       "      <td>5410.0</td>\n",
       "      <td>7198.0</td>\n",
       "      <td>3915.0</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>5185.0</td>\n",
       "      <td>46100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region    Fresh    Milk  Grocery  Frozen  Detergents_Paper  \\\n",
       "0        2       3  12669.0  9656.0   7561.0   214.0            2674.0   \n",
       "1        2       3   7057.0  9810.0   9568.0  1762.0            3293.0   \n",
       "2        2       3   6353.0  8808.0   7684.0  2405.0            3516.0   \n",
       "3        1       3  13265.0  1196.0   4221.0  6404.0             507.0   \n",
       "4        2       3  22615.0  5410.0   7198.0  3915.0            1777.0   \n",
       "\n",
       "   Delicassen  Total_Spend  labels  \n",
       "0      1338.0        34112       1  \n",
       "1      1776.0        33266       1  \n",
       "2      7844.0        36610       1  \n",
       "3      1788.0        27381       0  \n",
       "4      5185.0        46100       1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to the elbow we can choose 2 like the correct number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=2).fit(customers_scale)\n",
    "\n",
    "labels = kmeans_2.predict(customers_scale)\n",
    "\n",
    "clusters = kmeans_2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_customers['Label'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values:  {0, 1}\n",
      "Clusters count  labels\n",
      "0    249\n",
      "1    191\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cluster_counts = customers['labels'].value_counts()\n",
    "print('Unique values: ', set(clusters))\n",
    "print('Clusters count ', cluster_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   labels_DBSCAN\n",
      "0             -1\n",
      "1              0\n",
      "2             -1\n",
      "3             -1\n",
      "4             -1\n",
      "\n",
      "Cluster Distribution:\n",
      "labels_DBSCAN\n",
      "-1    435\n",
      " 0      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Step 1: Initialize the DBSCAN model\n",
    "# eps is the maximum distance between two samples to be considered as neighbors\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust min_samples if necessary\n",
    "\n",
    "# Step 2: Fit the DBSCAN model to the scaled data\n",
    "dbscan.fit(customers_scale)\n",
    "\n",
    "# Step 3: Retrieve the cluster labels\n",
    "# Add the DBSCAN cluster labels as a new column in the customers dataset\n",
    "customers_scale['labels_DBSCAN'] = dbscan.labels_\n",
    "\n",
    "# Step 4: Inspect the data with DBSCAN cluster assignments\n",
    "print(customers_scale[['labels_DBSCAN']].head())  # Display the first few rows of the DBSCAN cluster labels\n",
    "print(\"\\nCluster Distribution:\")\n",
    "print(customers_scale['labels_DBSCAN'].value_counts())  # Check how many customers are in each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values:  {0, 1}\n",
      "Clusters count  labels_DBSCAN\n",
      "-1    435\n",
      " 0      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cluster_counts = customers_scale['labels_DBSCAN'].value_counts()\n",
    "print('Unique values: ', set(clusters))\n",
    "print('Clusters count ', cluster_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x,y,hue,title):\n",
    "    sns.scatterplot(x=x, \n",
    "                    y=y,\n",
    "                    hue=hue, \n",
    "                    palette=\"viridis\",                     \n",
    "                    s=100, \n",
    "                    alpha=0.7)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the custom function to visualize DBSCAN clustering results\n",
    "# Use the corrected indexing for the columns in customers_scale DataFrame\n",
    "plot(\n",
    "    x=customers_scale['Detergents_Paper'],  # Scaled Detergents_Paper\n",
    "    y=customers_scale['Milk'],             # Scaled Milk\n",
    "    hue=customers_scale['labels_DBSCAN'],         # Cluster labels\n",
    "    title='Detergents Paper vs Milk'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Grocery vs Fresh by labels (K-Means)\n",
    "plot(\n",
    "    x=customers_scale['Grocery'],          # Scaled Grocery\n",
    "    y=customers_scale['Fresh'],            # Scaled Fresh\n",
    "    hue=customers['labels'],               # K-Means cluster labels\n",
    "    title='K-Means Clustering: Grocery vs Fresh'\n",
    ")\n",
    "\n",
    "# Visualize Grocery vs Fresh by labels_DBSCAN (DBSCAN)\n",
    "plot(\n",
    "    x=customers_scale['Grocery'],          # Scaled Grocery\n",
    "    y=customers_scale['Fresh'],            # Scaled Fresh\n",
    "    hue=customers_scale['labels_DBSCAN'],        # DBSCAN cluster labels\n",
    "    title='DBSCAN Clustering: Grocery vs Fresh'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Grocery vs Fresh by labels (K-Means)\n",
    "plot(\n",
    "    x=customers_scale['Frozen'],          # Scaled Grocery\n",
    "    y=customers_scale['Delicassen'],            # Scaled Fresh\n",
    "    hue=customers['labels'],               # K-Means cluster labels\n",
    "    title='K-Means Clustering: Grocery vs Fresh'\n",
    ")\n",
    "\n",
    "# Visualize Grocery vs Fresh by labels_DBSCAN (DBSCAN)\n",
    "plot(\n",
    "    x=customers_scale['Frozen'],          # Scaled Grocery\n",
    "    y=customers_scale['Delicassen'],            # Scaled Fresh\n",
    "    hue=customers_scale['labels_DBSCAN'],        # DBSCAN cluster labels\n",
    "    title='DBSCAN Clustering: Grocery vs Fresh'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Values by K-Means Labels:\n",
      "           Fresh      Milk   Grocery    Frozen  Detergents_Paper  Delicassen  \\\n",
      "labels                                                                         \n",
      "0       0.219228 -0.611488 -0.651790  0.278162         -0.669766   -0.147796   \n",
      "1      -0.285799  0.797175  0.849716 -0.362630          0.873150    0.192677   \n",
      "\n",
      "        labels_DBSCAN  \n",
      "labels                 \n",
      "0           -1.000000  \n",
      "1           -0.973822  \n",
      "\n",
      "Mean Values by DBSCAN Labels:\n",
      "                  Fresh      Milk   Grocery    Frozen  Detergents_Paper  \\\n",
      "labels_DBSCAN                                                             \n",
      "-1            -0.001344 -0.007572 -0.007642 -0.000144         -0.008165   \n",
      " 0             0.116885  0.658756  0.664844  0.012529          0.710353   \n",
      "\n",
      "               Delicassen    labels  \n",
      "labels_DBSCAN                        \n",
      "-1              -0.008349  0.427586  \n",
      " 0               0.726337  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Group by K-Means labels and compute the mean for all columns\n",
    "kmeans_group_means = customers_scale.groupby('labels').mean()\n",
    "print(\"Mean Values by K-Means Labels:\")\n",
    "print(kmeans_group_means)\n",
    "\n",
    "# Group by DBSCAN labels and compute the mean for all columns\n",
    "dbscan_group_means = customers_scale.groupby('labels_DBSCAN').mean()\n",
    "print(\"\\nMean Values by DBSCAN Labels:\")\n",
    "print(dbscan_group_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### K-Means:\n",
    "- Appears to perform better in this scenario.\n",
    "- Successfully identified two distinct customer groups with interpretable characteristics.\n",
    "\n",
    "#### DBSCAN:\n",
    "- Could perform better with optimized parameters (eps, min_samples) but currently overclassified data as noise.\n",
    "- Suitable for datasets with irregularly shaped clusters or significant noise, but adjustments are needed to uncover meaningful clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "import pandas as pd\n",
    "\n",
    "# Important, use the scale data \"customers_scale\"\n",
    "\n",
    "# Multiple plots configuration\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "# Loop throught different clusters (from 2 to 5)\n",
    "for i, k in enumerate([2, 3, 4]):\n",
    "    # Create the KMeans intances for each cluster\n",
    "    km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
    "\n",
    "    # plot position\n",
    "    row, col = divmod(i, 2)\n",
    "\n",
    "    # Create the SilhouetteVisualizer with the KMeans instance\n",
    "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[row][col])\n",
    "\n",
    "    # Ajust the visualizar to data scaled\n",
    "    visualizer.fit(customers_scale)\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate the KMeans model\n",
    "# random_state=42 is used for reproducibility of results\n",
    "km = KMeans(random_state=42)\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the KMeans model\n",
    "# k=(2,10) indicates the range of number of clusters to try (from 2 to 10)\n",
    "visualizer = KElbowVisualizer(km, k=(2,10))\n",
    "\n",
    "# Fit the visualizer to the data\n",
    "# This will run K-means clustering for each value of k and calculate the distortion score for each\n",
    "visualizer.fit(customers_scale)\n",
    "\n",
    "# Render the plot\n",
    "# The Elbow plot displays the distortion score for each k\n",
    "# The point where the distortion score starts to level off ('elbow') is the recommended number of clusters\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels Assigned to Customers:\n",
      "   labels\n",
      "0       0\n",
      "1       1\n",
      "2       1\n",
      "3       2\n",
      "4       1\n",
      "\n",
      "Cluster Distribution:\n",
      "labels\n",
      "2    150\n",
      "1    139\n",
      "3     99\n",
      "0     52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the K-Means model\n",
    "# Based on the previous analysis the clusters will be 4\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "# Step 2: Fit the K-Means model to the scaled data\n",
    "kmeans.fit(customers_scale)\n",
    "\n",
    "# Step 3: Retrieve the cluster labels\n",
    "customers_scale['labels'] = kmeans.labels_\n",
    "\n",
    "# Step 5: Display the results\n",
    "print(\"Cluster Labels Assigned to Customers:\")\n",
    "print(customers_scale[['labels']].head())\n",
    "\n",
    "# Step 6: Analyze Cluster Distribution\n",
    "print(\"\\nCluster Distribution:\")\n",
    "print(customers_scale['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Grocery vs Fresh by K-Mean\n",
    "plot(\n",
    "    x=customers_scale['Milk'],          # Scaled Grocery\n",
    "    y=customers_scale['Fresh'],            # Scaled Fresh\n",
    "    hue=customers_scale['labels'],         # labels\n",
    "    title='K-Means Clustering Results'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Results:\n",
    "- The K-Elbow plot identified the optimal number of clusters as 4.\n",
    "- After implementing 4 clusters, the distribution was:\n",
    "    - Cluster 0: 52 points (10.7%)\n",
    "    - Cluster 1: 139 points (28.7%)\n",
    "    - Cluster 2: 150 points (31.0%)\n",
    "    - Cluster 3: 99 points (20.5%)\n",
    "- Plot Insights:\n",
    "    - The additional clusters (compared to 2 clusters) appear to better capture variability in the data, particularly for points that were previously lumped together in Cluster 1.\n",
    "    - This setup likely improves granularity and differentiation among groups.\n",
    "- Key Observations:\n",
    "    - Having four clusters provides more nuanced segmentation of the dataset, but care should be taken to ensure that these clusters are meaningful (e.g., distinct groups of customers).\n",
    "### Silhouette Visualizations (2-5 Clusters):\n",
    "- Clustering Quality:\n",
    "    - From the silhouette plots, it’s evident that some clusters have higher compactness and separation than others.\n",
    "    - For example, in 2 clusters, the silhouette scores are generally high, but there are wide variations for some points.\n",
    "    - As the number of clusters increases to 4 or 5, the silhouette widths narrow, which suggests diminishing returns in cluster separation with more clusters.\n",
    "- Key Observations:\n",
    "    - The ideal cluster number balances silhouette score (higher is better) and the granularity of insights.\n",
    "    - Based on the visualizations, 4 clusters seem to be a sweet spot for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Sample dataset (replace with your data)\n",
    "from sklearn.datasets import make_blobs\n",
    "data, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.5, random_state=42)\n",
    "\n",
    "# Define the value of k (usually equal to min_samples in DBSCAN)\n",
    "k = 5\n",
    "\n",
    "# Compute k-nearest neighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=k).fit(data)\n",
    "distances, indices = nbrs.kneighbors(data)\n",
    "\n",
    "# Sort the distances to the k-th nearest neighbor\n",
    "k_distances = np.sort(distances[:, k-1])\n",
    "\n",
    "# Plot the k-distance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_distances)\n",
    "plt.title('k-Distance Graph')\n",
    "plt.xlabel('Points sorted by distance to k-th nearest neighbor')\n",
    "plt.ylabel(f'{k}-th Nearest Neighbor Distance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   labels_DBSCAN_08\n",
      "0                -1\n",
      "1                 0\n",
      "2                -1\n",
      "3                -1\n",
      "4                -1\n",
      "\n",
      "Cluster Distribution:\n",
      "labels_DBSCAN_08\n",
      "-1     306\n",
      " 1      40\n",
      " 4      23\n",
      " 7      13\n",
      " 8      10\n",
      " 9       8\n",
      " 3       7\n",
      " 5       7\n",
      " 10      6\n",
      " 0       5\n",
      " 2       5\n",
      " 6       5\n",
      " 11      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the DBSCAN model\n",
    "# eps is the maximum distance between two samples to be considered as neighbors\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=5)  # Adjust min_samples if necessary\n",
    "\n",
    "# Step 2: Fit the DBSCAN model to the scaled data\n",
    "dbscan.fit(customers_scale)\n",
    "\n",
    "# Step 3: Retrieve the cluster labels\n",
    "# Add the DBSCAN cluster labels as a new column in the customers dataset\n",
    "customers_scale['labels_DBSCAN_08'] = dbscan.labels_\n",
    "\n",
    "# Step 4: Inspect the data with DBSCAN cluster assignments\n",
    "print(customers_scale[['labels_DBSCAN_08']].head())  # Display the first few rows of the DBSCAN cluster labels\n",
    "print(\"\\nCluster Distribution:\")\n",
    "print(customers_scale['labels_DBSCAN_08'].value_counts())  # Check how many customers are in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    x=customers_scale['Milk'],          # Scaled Grocery\n",
    "    y=customers_scale['Fresh'],            # Scaled Fresh\n",
    "    hue=customers_scale['labels_DBSCAN_08'],         # labels\n",
    "    title='DBScan Clustering Results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score for eps=0.5 cannot be calculated (only one cluster or all noise).\n",
      "Silhouette Score for eps=0.8: -0.08904220744615556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Ensure X is defined\n",
    "X = customers_scale[['Milk', 'Fresh']].values\n",
    "\n",
    "# For eps=0.5\n",
    "filtered_X_05 = X[customers_scale['labels_DBSCAN'] != -1]\n",
    "filtered_labels_05 = customers_scale['labels_DBSCAN'][customers_scale['labels_DBSCAN'] != -1]\n",
    "\n",
    "if len(set(filtered_labels_05)) > 1:  # Check if there are at least two clusters\n",
    "    score_05 = silhouette_score(filtered_X_05, filtered_labels_05)\n",
    "    print(f'Silhouette Score for eps=0.5: {score_05}')\n",
    "else:\n",
    "    print(\"Silhouette Score for eps=0.5 cannot be calculated (only one cluster or all noise).\")\n",
    "\n",
    "# For eps=0.8\n",
    "filtered_X_08 = X[customers_scale['labels_DBSCAN_08'] != -1]\n",
    "filtered_labels_08 = customers_scale['labels_DBSCAN_08'][customers_scale['labels_DBSCAN_08'] != -1]\n",
    "\n",
    "if len(set(filtered_labels_08)) > 1:  # Check if there are at least two clusters\n",
    "    score_08 = silhouette_score(filtered_X_08, filtered_labels_08)\n",
    "    print(f'Silhouette Score for eps=0.8: {score_08}')\n",
    "else:\n",
    "    print(\"Silhouette Score for eps=0.8 cannot be calculated (only one cluster or all noise).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations and Insights\n",
    "#### A. Noise Points\n",
    "- eps=0.5: A significant portion of the points (99%) were classified as noise, suggesting that the neighborhood radius was too small to form clusters effectively.\n",
    "- eps=0.8: The noise reduced to 305 points (~69%), which shows that increasing eps allowed more points to form clusters.\n",
    "\n",
    "#### B. Cluster Formation\n",
    "- eps=0.5: Only one cluster (Cluster 0) was formed, with just 5 points. This highlights that the threshold for forming a cluster was too restrictive.\n",
    "- eps=0.8: Eleven clusters were identified, ranging from 5 to 46 points. This is a more meaningful partitioning of the data but still leaves many points classified as noise.\n",
    "\n",
    "#### C. Largest Cluster\n",
    "- eps=0.5: The largest cluster had only 5 points, which isn't enough to represent a meaningful group.\n",
    "- eps=0.8: The largest cluster grew to 46 points, indicating that the increased eps allowed more data points to connect and form clusters.\n",
    "\n",
    "#### D. Cluster Diversity\n",
    "- eps=0.5: There was no diversity in clustering; all points either belonged to a single small cluster or were noise.\n",
    "- eps=0.8: Multiple clusters were identified, suggesting that the new eps is closer to capturing the natural structure of the data. However, some clusters are still very small (5-13 points)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
